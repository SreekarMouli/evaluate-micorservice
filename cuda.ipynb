{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZP6WyqnyCsU",
        "outputId": "75851960-6001-4ff8-dfaf-40e5d796c428"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aHmpY1RyPqm",
        "outputId": "02ad82d1-7f35-4e5c-da80-4e3462920d39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-h9zxrea_\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-h9zxrea_\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit 0a71d56e5dce3ff1f0dd2c47c29367629262f527\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4295 sha256=d7e647ce88fd0580ac78dec8c954d6f4b650389bac3953712dcdd55fca3260ca\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-11fq2u17/wheels/a8/b9/18/23f8ef71ceb0f63297dd1903aedd067e6243a68ea756d6feea\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMKTvGaDyWnV",
        "outputId": "6468a58d-81ac-4660-d626-bb42d5e26902"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install nlohmann-json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emPEYnz00OaB",
        "outputId": "b4f10b2d-0ddf-48bb-d46d-5d8e871fe7d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nlohmann-json\n",
            "  Downloading nlohmann_json-3.11.2-py3-none-any.whl (167 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/167.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/167.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.1/167.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from nlohmann-json) (6.0.1)\n",
            "Installing collected packages: nlohmann-json\n",
            "Successfully installed nlohmann-json-3.11.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "/* Evaluate Service written in cuda-cpp for execution on Nvidia GPUs  */\n",
        "#include <iostream>\n",
        "#include <nlohmann/json.hpp>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "using json = nlohmann::json;\n",
        "\n",
        "__device__ float plus(float a, float b) {\n",
        "    return a + b;\n",
        "}\n",
        "\n",
        "__device__ float minus(float a, float b) {\n",
        "    return a - b;\n",
        "}\n",
        "\n",
        "__device__ float multiply(float a, float b) {\n",
        "    return a * b;\n",
        "}\n",
        "\n",
        "__device__ float handle_variable(const json& row, const std::string& key) {\n",
        "    if (row.find(key) != row.end()) {\n",
        "        return row[key].get<float>();\n",
        "    }\n",
        "    return 0.0; // Default value if variable is not found\n",
        "}\n",
        "\n",
        "template <typename T>\n",
        "__global__ void evaluateFormula(const char* formulaJson, const char* rowsJson, T* results, int numRows) {\n",
        "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\n",
        "    if (tid < numRows) {\n",
        "        json ast = json::parse(formulaJson);\n",
        "        json row = json::parse(rowsJson); // Parse the row as a JSON object\n",
        "\n",
        "        // Parse the JSON AST dynamically and evaluate the formula for row 'tid'\n",
        "        T result = 0.0; // Initialize with the neutral element\n",
        "\n",
        "        // Traverse the JSON AST and apply operations dynamically\n",
        "        for (const auto& item : ast) {\n",
        "            if (item.is_object()) {\n",
        "                const std::string& operator_str = item.begin().key();\n",
        "                const json& operands = item.begin().value();\n",
        "\n",
        "                if (operator_str == \"+\") {\n",
        "                    result = plus(result, operands[0].is_string() ? handle_variable(row, operands[0]) : operands[0].get<float>());\n",
        "                } else if (operator_str == \"-\") {\n",
        "                    result = minus(result, operands[0].is_string() ? handle_variable(row, operands[0]) : operands[0].get<float>());\n",
        "                } else if (operator_str == \"*\") {\n",
        "                    result = multiply(result, operands[0].is_string() ? handle_variable(row, operands[0]) : operands[0].get<float>());\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        results[tid] = result; // Store the result in the results array\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Initialize CUDA\n",
        "    cudaError_t cudaStatus = cudaSetDevice(0);\n",
        "    if (cudaStatus != cudaSuccess) {\n",
        "        std::cerr << \"CUDA initialization failed!\" << std::endl;\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    // Define problem size\n",
        "    int numRows = 1; // Adjust as needed\n",
        "    int blockSize = 1; // Adjust as needed\n",
        "\n",
        "    // Allocate memory on the CPU for results\n",
        "    float* results = new float[numRows];\n",
        "\n",
        "    // Convert the JSON AST and row data to strings\n",
        "    std::string formulaJson = \"{\\\"+\\\": [\\\"a\\\", {\\\"-\\\": [\\\"b\\\", {\\\"*\\\": [\\\"c\\\", \\\"d\\\"]}]}]}\";\n",
        "    std::string rowJson = \"{\\\"a\\\": 1.0, \\\"b\\\": 2.0, \\\"c\\\": 3.0, \\\"d\\\": 4.0}\";\n",
        "\n",
        "    const char* formulaJsonCStr = formulaJson.c_str();\n",
        "    const char* rowJsonCStr = rowJson.c_str();\n",
        "\n",
        "    // Allocate memory on the GPU for results\n",
        "    float* d_results;\n",
        "    cudaStatus = cudaMalloc((void**)&d_results, numRows * sizeof(float));\n",
        "    if (cudaStatus != cudaSuccess) {\n",
        "        std::cerr << \"CUDA memory allocation failed!\" << std::endl;\n",
        "        delete[] results;\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    // Launch the CUDA kernel\n",
        "    evaluateFormula<float><<<numRows, blockSize>>>(formulaJsonCStr, rowJsonCStr, d_results, numRows);\n",
        "\n",
        "    // Check for kernel launch errors\n",
        "    cudaStatus = cudaGetLastError();\n",
        "    if (cudaStatus != cudaSuccess) {\n",
        "        std::cerr << \"CUDA kernel launch failed: \" << cudaGetErrorString(cudaStatus) << std::endl;\n",
        "        cudaFree(d_results);\n",
        "        delete[] results;\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    // Copy results from GPU to CPU\n",
        "    cudaStatus = cudaMemcpy(results, d_results, numRows * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "    if (cudaStatus != cudaSuccess) {\n",
        "        std::cerr << \"CUDA memcpy failed: \" << cudaGetErrorString(cudaStatus) << std::endl;\n",
        "        cudaFree(d_results);\n",
        "        delete[] results;\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    // Print or use the result\n",
        "    std::cout << \"Result: \" << results[0] << std::endl;\n",
        "\n",
        "    // Cleanup\n",
        "    cudaFree(d_results);\n",
        "    delete[] results;\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nh-oP73oyghX",
        "outputId": "e3395df3-1168-46c2-baee-0c2a48a1e415"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp/tmp_j6tbnhe/1478df41-44ff-4e53-8122-235aaf64df69.cu:3:10: fatal error: nlohmann/json.hpp: No such file or directory\n",
            "    3 | #include <nlohmann/json.hpp>\n",
            "      |          ^~~~~~~~~~~~~~~~~~~\n",
            "compilation terminated.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cupy as cp\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Function to perform matrix multiplication using CuPy on GPU\n",
        "def matrix_multiply_gpu(A, B):\n",
        "    # Move data to the GPU\n",
        "    A_gpu = cp.asarray(A)\n",
        "    B_gpu = cp.asarray(B)\n",
        "\n",
        "    # Perform matrix multiplication on the GPU\n",
        "    C_gpu = cp.matmul(A_gpu, B_gpu)\n",
        "\n",
        "    # Move the result back to the CPU\n",
        "    C_cpu = cp.asnumpy(C_gpu)\n",
        "\n",
        "    return C_cpu\n",
        "\n",
        "# Function to perform matrix multiplication using NumPy on CPU\n",
        "def matrix_multiply_cpu(A, B):\n",
        "    return np.matmul(A, B)\n",
        "\n",
        "# Create two random matrices (adjust the size as needed)\n",
        "matrix_size = (10000, 10000)\n",
        "A = np.random.rand(*matrix_size).astype(np.float32)\n",
        "B = np.random.rand(*matrix_size).astype(np.float32)\n",
        "\n",
        "# Measure the time taken by CuPy on GPU\n",
        "start_time_gpu = time.time()\n",
        "result_gpu = matrix_multiply_gpu(A, B)\n",
        "end_time_gpu = time.time()\n",
        "\n",
        "# Measure the time taken by NumPy on CPU\n",
        "start_time_cpu = time.time()\n",
        "result_cpu = matrix_multiply_cpu(A, B)\n",
        "end_time_cpu = time.time()\n",
        "\n",
        "# Compare the results (for correctness)\n",
        "if np.allclose(result_gpu, result_cpu):\n",
        "    print(\"Results match.\")\n",
        "else:\n",
        "    print(\"Results do not match.\")\n",
        "\n",
        "# Compare the performance\n",
        "time_gpu = end_time_gpu - start_time_gpu\n",
        "time_cpu = end_time_cpu - start_time_cpu\n",
        "\n",
        "print(f\"Time taken by CuPy on GPU: {time_gpu:.4f} seconds\")\n",
        "print(f\"Time taken by NumPy on CPU: {time_cpu:.4f} seconds\")\n",
        "\n",
        "# Calculate the speedup (GPU time / CPU time)\n",
        "speedup = time_cpu / time_gpu\n",
        "print(f\"Speedup: {speedup:.2f}x\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObnqUhw60VRk",
        "outputId": "bc71e975-c1c6-4fbc-ba98-c89740ca2ae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results match.\n",
            "Time taken by CuPy on GPU: 1.2838 seconds\n",
            "Time taken by NumPy on CPU: 29.6650 seconds\n",
            "Speedup: 23.11x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cupy as cp\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Function to perform element-wise multiplication using CuPy on GPU\n",
        "def elementwise_multiply_gpu(vector, number):\n",
        "    # Move data to the GPU\n",
        "    vector_gpu = cp.asarray(vector)\n",
        "    number_gpu = cp.asarray(number)\n",
        "\n",
        "    # Perform element-wise multiplication on the GPU\n",
        "    result_gpu = cp.multiply(vector_gpu, number_gpu)\n",
        "\n",
        "    # Move the result back to the CPU\n",
        "    result_cpu = cp.asnumpy(result_gpu)\n",
        "\n",
        "    return result_cpu\n",
        "\n",
        "# Function to perform element-wise multiplication using NumPy on CPU\n",
        "def elementwise_multiply_cpu(vector, number):\n",
        "    return vector * number\n",
        "\n",
        "# Create a random vector (adjust the length as needed)\n",
        "n = 100000000  # Length of the vector\n",
        "vector = np.random.rand(n).astype(np.float32)\n",
        "number = 2.0  # Number to multiply with\n",
        "\n",
        "# Measure the time taken by CuPy on GPU\n",
        "start_time_gpu = time.time()\n",
        "result_gpu = elementwise_multiply_gpu(vector, number)\n",
        "end_time_gpu = time.time()\n",
        "\n",
        "# Measure the time taken by NumPy on CPU\n",
        "start_time_cpu = time.time()\n",
        "result_cpu = elementwise_multiply_cpu(vector, number)\n",
        "end_time_cpu = time.time()\n",
        "\n",
        "# Compare the results (for correctness)\n",
        "if np.allclose(result_gpu, result_cpu):\n",
        "    print(\"Results match.\")\n",
        "else:\n",
        "    print(\"Results do not match.\")\n",
        "\n",
        "# Compare the performance\n",
        "time_gpu = end_time_gpu - start_time_gpu\n",
        "time_cpu = end_time_cpu - start_time_cpu\n",
        "\n",
        "print(f\"Time taken by CuPy on GPU: {time_gpu:.4f} seconds\")\n",
        "print(f\"Time taken by NumPy on CPU: {time_cpu:.4f} seconds\")\n",
        "\n",
        "# Calculate the speedup (CPU time / GPU time)\n",
        "speedup = time_cpu / time_gpu\n",
        "print(f\"Speedup: {speedup:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAiRrrgzWWXk",
        "outputId": "8559578e-1075-4988-dec2-2cf4044f0630"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results match.\n",
            "Time taken by CuPy on GPU: 0.4395 seconds\n",
            "Time taken by NumPy on CPU: 0.1313 seconds\n",
            "Speedup: 0.30\n"
          ]
        }
      ]
    }
  ]
}